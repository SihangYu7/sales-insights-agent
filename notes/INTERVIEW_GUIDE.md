# AI Agent Project - Tech Interview Guide

## üéØ Quick Project Summary (30-Second Pitch)

"I built an AI-powered sales analytics assistant using Flask, SQLite, and OpenAI's GPT models. The system accepts natural language questions about sales data, and the next phase will convert those questions into SQL queries, execute them against a database, and return intelligent responses. It demonstrates full-stack development, AI integration, database design, and REST API architecture."

---

## üìã Table of Contents
1. [Project Overview](#project-overview)
2. [Architecture & Tech Stack](#architecture--tech-stack)
3. [Module 4 Focus - Current Work](#module-4-focus---current-work)
4. [Code Walkthrough](#code-walkthrough)
5. [Demo Script](#demo-script)
6. [Technical Deep Dives](#technical-deep-dives)
7. [Common Interview Questions](#common-interview-questions)

---

## üèóÔ∏è Project Overview

### What Problem Does It Solve?
Business users need insights from sales data but lack SQL knowledge. This AI agent bridges that gap by:
- Converting natural language to SQL queries
- Executing queries safely against a sales database
- Returning human-friendly answers with business context

### Project Status
- ‚úÖ **Module 1**: Flask REST API with 6 endpoints
- ‚úÖ **Module 2**: SQLite database with 200+ sales records
- ‚úÖ **Module 3**: OpenAI GPT integration for Q&A
- üöß **Module 4**: Text-to-SQL generation (IN PROGRESS)

### Learning Objectives
This project teaches:
- REST API design with Flask
- Database modeling and SQLAlchemy ORM
- OpenAI API integration
- Prompt engineering for AI agents
- Security (SQL injection prevention)
- Error handling and production patterns

---

## üîß Architecture & Tech Stack

### System Architecture
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Client (Browser/CLI)                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ HTTP/JSON
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Flask API Layer (app.py)                  ‚îÇ
‚îÇ  Routes: /, /api/products, /api/sales/summary,              ‚îÇ
‚îÇ          /api/schema, /api/query, /api/ask                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ                      ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ  database.py      ‚îÇ   ‚îÇ openai_service.py‚îÇ
         ‚îÇ  SQLAlchemy ORM   ‚îÇ   ‚îÇ GPT Integration  ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ                      ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ   SQLite DB       ‚îÇ   ‚îÇ   OpenAI API    ‚îÇ
         ‚îÇ   sales.db        ‚îÇ   ‚îÇ  gpt-3.5-turbo  ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Tech Stack

**Backend**
- **Framework**: Flask 3.0.0
- **Database**: SQLite + SQLAlchemy 2.0.45
- **AI**: OpenAI SDK 2.15.0+, LangChain 1.2.6+
- **Language**: Python 3.14

**Key Dependencies**
```python
flask==3.0.0           # Web framework
flask-cors==4.0.0      # CORS support
openai>=2.15.0         # OpenAI GPT models
langchain>=1.2.6       # AI agent framework (for Module 4)
sqlalchemy>=2.0.45     # Database ORM
python-dotenv==1.0.0   # Environment variables
```

### Database Schema

**Products Table**
```sql
CREATE TABLE products (
    id INTEGER PRIMARY KEY,
    name TEXT NOT NULL,
    category TEXT NOT NULL,  -- Electronics, Furniture, Books
    price REAL NOT NULL
);
```

**Sales Table**
```sql
CREATE TABLE sales (
    id INTEGER PRIMARY KEY,
    product_id INTEGER,
    quantity INTEGER NOT NULL,
    total REAL NOT NULL,
    sale_date TEXT NOT NULL,
    region TEXT NOT NULL,    -- North, South, East, West
    FOREIGN KEY (product_id) REFERENCES products(id)
);
```

**Sample Data**
- 10 products across 3 categories
- 200 sales records
- 4 regions (North, South, East, West)
- 90 days of historical data

---

## üöÄ Module 4 Focus - Current Work

### What is Module 4?
**Module 4: Text-to-SQL Generation**

The core AI agent capability - converting natural language questions into executable SQL queries.

### Current Implementation Challenge

**Problem**: Module 3 AI assistant can answer general business questions but can't access actual data.

**Module 4 Solution**:
1. User asks: "What are total sales in the North region?"
2. AI reads database schema from `/api/schema`
3. AI generates SQL: `SELECT SUM(total) FROM sales WHERE region = 'North'`
4. System executes SQL safely via `/api/query`
5. AI formats result: "Total sales in North region: $45,320.50"

### Key Technical Concepts

**1. Schema Injection**
```python
def get_schema_info():
    """Provides AI with database structure"""
    return {
        "tables": {
            "products": {
                "columns": ["id", "name", "category", "price"],
                "sample_query": "SELECT * FROM products LIMIT 5"
            },
            "sales": {
                "columns": ["id", "product_id", "quantity", "total",
                           "sale_date", "region"],
                "sample_query": "SELECT * FROM sales LIMIT 5"
            }
        }
    }
```

**2. Prompt Engineering for SQL Generation**
```python
system_prompt = """You are a SQL expert assistant.
Database schema:
{schema_info}

Generate SQL SELECT queries based on user questions.
Rules:
- Only generate SELECT queries
- Use proper JOIN syntax for related tables
- Include column aliases for clarity
- Return ONLY the SQL query, no explanation
"""
```

**3. SQL Safety Validation**
```python
def is_safe_query(sql: str) -> bool:
    """Prevent dangerous SQL operations"""
    forbidden = ['DROP', 'DELETE', 'INSERT', 'UPDATE',
                 'ALTER', 'CREATE', 'TRUNCATE']
    return not any(word in sql.upper() for word in forbidden)
```

### Module 4 Implementation Steps

1. **Enhance OpenAI prompts** with database schema
2. **Add SQL generation function** in `openai_service.py`
3. **Validate generated SQL** before execution
4. **Execute query** via existing `/api/query` endpoint
5. **Format results** into human-readable responses

---

## üíª Code Walkthrough

### File Structure
```
ai-agent/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ app.py                 # Flask API (219 lines)
‚îÇ   ‚îú‚îÄ‚îÄ database.py            # SQLAlchemy models (235 lines)
‚îÇ   ‚îú‚îÄ‚îÄ openai_service.py      # OpenAI integration (306 lines)
‚îÇ   ‚îú‚îÄ‚îÄ sales.db               # SQLite database (20KB)
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt       # Dependencies
‚îÇ   ‚îî‚îÄ‚îÄ .env                   # API keys (OPENAI_API_KEY)
‚îî‚îÄ‚îÄ venv/                      # Virtual environment
```

### 1. Flask API Layer (`app.py`)

**Location**: `/Users/yus6/Documents/Courses/cs146s/agent/ai-agent/backend/app.py`

**Key Endpoints**:

```python
# Health check
@app.route('/', methods=['GET'])
def health_check():
    return jsonify({
        "status": "healthy",
        "message": "AI Agent Backend is running! üöÄ"
    })

# Get all products
@app.route('/api/products', methods=['GET'])
def get_products():
    db = get_db()
    products = db.query(Product).all()
    return jsonify({
        "count": len(products),
        "products": [p.to_dict() for p in products]
    })

# Sales analytics dashboard
@app.route('/api/sales/summary', methods=['GET'])
def get_sales_summary():
    """Aggregated sales data with JOINs and GROUP BY"""
    # Returns: total sales, sales by region, sales by category, top products

# Database schema (for AI)
@app.route('/api/schema', methods=['GET'])
def get_schema():
    """Returns database structure for AI to understand available data"""
    return jsonify(get_schema_info())

# Execute raw SQL (with safety checks)
@app.route('/api/query', methods=['POST'])
def execute_query():
    """Execute user SQL queries (SELECT only)"""
    sql = request.json.get('sql')

    # Security: Block dangerous operations
    forbidden = ['DROP', 'DELETE', 'INSERT', 'UPDATE', 'ALTER', 'CREATE']
    if any(word in sql.upper() for word in forbidden):
        return jsonify({"error": "Only SELECT queries allowed"}), 400

    results = run_query(sql)
    return jsonify(results)

# AI-powered Q&A
@app.route('/api/ask', methods=['POST'])
def ask():
    """Natural language question ‚Üí AI response"""
    question = request.json.get('question')
    result = ai_sales_assistant(question)
    return jsonify(result)
```

**Interview Talking Point**:
"I designed a clean REST API with 6 endpoints, each serving a specific purpose. The `/api/query` endpoint includes SQL injection prevention, and the `/api/schema` endpoint provides machine-readable schema information for the AI agent to understand what data is available."

---

### 2. Database Layer (`database.py`)

**Location**: `/Users/yus6/Documents/Courses/cs146s/agent/ai-agent/backend/database.py`

**Key Functions**:

```python
# SQLAlchemy Models
class Product(Base):
    __tablename__ = 'products'
    id = Column(Integer, primary_key=True)
    name = Column(String, nullable=False)
    category = Column(String, nullable=False)
    price = Column(Float, nullable=False)

    def to_dict(self):
        return {
            "id": self.id,
            "name": self.name,
            "category": self.category,
            "price": self.price
        }

class Sale(Base):
    __tablename__ = 'sales'
    id = Column(Integer, primary_key=True)
    product_id = Column(Integer, ForeignKey('products.id'))
    quantity = Column(Integer, nullable=False)
    total = Column(Float, nullable=False)
    sale_date = Column(String, nullable=False)
    region = Column(String, nullable=False)

# Database initialization
def init_db():
    """Create all tables"""
    Base.metadata.create_all(engine)

# Seed sample data
def seed_database():
    """Populate database with 200 sales records"""
    # 10 products, 200 sales across 4 regions

# Execute raw SQL
def run_query(sql_query):
    """Execute arbitrary SQL and return results as list of dicts"""
    with engine.connect() as conn:
        result = conn.execute(text(sql_query))
        return [dict(row._mapping) for row in result]

# Schema information for AI
def get_schema_info():
    """Returns structured schema data for AI context"""
    return {
        "database_type": "SQLite",
        "tables": {
            "products": {...},
            "sales": {...}
        }
    }
```

**Interview Talking Point**:
"I used SQLAlchemy ORM for clean Python-to-database mapping, but also kept a raw SQL execution function for flexibility. The `get_schema_info()` function formats database structure in a way the AI can understand, including sample queries and column descriptions."

---

### 3. OpenAI Integration (`openai_service.py`)

**Location**: `/Users/yus6/Documents/Courses/cs146s/agent/ai-agent/backend/openai_service.py`

**Key Functions**:

```python
# Basic chat completion
def simple_chat(user_message, system_prompt="You are a helpful assistant."):
    """
    Basic OpenAI chat completion.

    Args:
        user_message: User's question
        system_prompt: AI's behavior instructions

    Returns:
        {
            "answer": "AI response",
            "success": True,
            "tokens": {"prompt": 45, "completion": 78, "total": 123}
        }
    """
    try:
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ],
            max_tokens=500,
            temperature=0.7
        )

        return {
            "answer": response.choices[0].message.content,
            "success": True,
            "tokens": {
                "prompt": response.usage.prompt_tokens,
                "completion": response.usage.completion_tokens,
                "total": response.usage.total_tokens
            }
        }
    except Exception as e:
        return {"answer": f"Error: {str(e)}", "success": False}

# Multi-turn conversation
def chat_with_context(user_message, conversation_history=[]):
    """
    Chat with conversation memory.

    Args:
        conversation_history: List of previous messages
            [{"role": "user", "content": "Hi"},
             {"role": "assistant", "content": "Hello!"}]
    """
    messages = conversation_history + [
        {"role": "user", "content": user_message}
    ]
    # ... call OpenAI API ...

# Sales assistant
def ai_sales_assistant(user_question):
    """
    Specialized AI for sales questions.
    Currently answers general questions.
    Module 4 will add SQL generation!
    """
    system_prompt = """You are a helpful sales data assistant.
    Your role:
    - Answer questions about sales, products, and business data
    - Be concise and professional
    - If you don't have the data to answer, say so clearly
    - Provide actionable insights when possible
    """

    return simple_chat(user_question, system_prompt)
```

**Interview Talking Point**:
"I built three levels of OpenAI integration: basic chat, conversation with memory, and a specialized sales assistant. The code includes comprehensive error handling for API failures, rate limits, and network issues. I track token usage to monitor costs - GPT-3.5-turbo costs about $0.0015 per 1,000 tokens."

---

## üé¨ Demo Script (5-7 Minutes)

### Setup (30 seconds)
```bash
cd /Users/yus6/Documents/Courses/cs146s/agent/ai-agent/backend
source ../venv/bin/activate
python app.py
```

**Say**: "Let me start the Flask backend. It initializes the SQLite database with sample data on startup."

---

### Demo 1: Health Check (30 seconds)
```bash
curl http://localhost:5000/
```

**Expected Output**:
```json
{
  "status": "healthy",
  "message": "AI Agent Backend is running! üöÄ",
  "modules_completed": [
    "Module 1: Flask API",
    "Module 2: Database",
    "Module 3: OpenAI Integration"
  ]
}
```

**Say**: "First, verify the server is running and see which modules are complete."

---

### Demo 2: Database Query (1 minute)
```bash
# Get all products
curl http://localhost:5000/api/products

# Get sales summary
curl http://localhost:5000/api/sales/summary
```

**Expected**: JSON with products and aggregated sales data

**Say**: "The `/api/products` endpoint demonstrates basic database retrieval. The `/api/sales/summary` endpoint shows more complex SQL with JOINs and GROUP BY aggregations - it calculates total sales, breaks down by region and category, and identifies top products."

---

### Demo 3: Database Schema (1 minute)
```bash
curl http://localhost:5000/api/schema
```

**Expected**: Structured schema information

**Say**: "This endpoint returns database structure in a format the AI can understand. It includes table names, columns, data types, relationships, and example queries. Module 4 will inject this schema into AI prompts so it knows what data exists and how to query it."

---

### Demo 4: Raw SQL Execution (1 minute)
```bash
# Safe query
curl -X POST http://localhost:5000/api/query \
  -H "Content-Type: application/json" \
  -d '{"sql": "SELECT category, COUNT(*) as product_count FROM products GROUP BY category"}'

# Blocked query (demonstrates security)
curl -X POST http://localhost:5000/api/query \
  -H "Content-Type: application/json" \
  -d '{"sql": "DROP TABLE products"}'
```

**Expected**: First query returns data, second returns error

**Say**: "The `/api/query` endpoint executes SQL but includes security checks. It blocks DELETE, DROP, INSERT, UPDATE, and other dangerous operations - only SELECT queries are allowed. This demonstrates SQL injection prevention."

---

### Demo 5: AI Q&A (2 minutes)
```bash
# General business question
curl -X POST http://localhost:5000/api/ask \
  -H "Content-Type: application/json" \
  -d '{"question": "What are the key metrics I should track for a sales team?"}'

# Data-specific question (shows limitation)
curl -X POST http://localhost:5000/api/ask \
  -H "Content-Type: application/json" \
  -d '{"question": "What are my total sales in the North region?"}'
```

**Expected**: First query gets intelligent response, second highlights need for Module 4

**Say**: "The AI assistant can answer general business questions using GPT-3.5-turbo. However, when you ask about specific data in the database, it can't access it yet. That's what Module 4 will solve - the AI will generate SQL queries to retrieve actual data and provide data-driven answers."

---

### Demo 6: Module 4 Preview (1 minute)

**Say**: "Here's what Module 4 will enable..."

**Show the planned flow**:
```
User asks: "What are total sales in the North region?"

Step 1: AI reads schema from /api/schema
Step 2: AI generates SQL:
        "SELECT SUM(total) FROM sales WHERE region = 'North'"
Step 3: System validates SQL (no DROP, DELETE, etc.)
Step 4: Execute via /api/query
Step 5: AI formats result:
        "Total sales in North region: $45,320.50"
```

**Say**: "This demonstrates the full lifecycle of an AI agent: understanding the question, reasoning about available tools (the database), taking action (generating and executing SQL), and presenting results in a human-friendly format."

---

## üéì Technical Deep Dives

### 1. Why GPT-3.5-Turbo vs GPT-4?

**Answer**:
"I chose GPT-3.5-turbo for this project because:
- **Cost**: 10-20x cheaper than GPT-4 ($0.0015 vs $0.03 per 1K tokens)
- **Speed**: 2-3x faster response times
- **Sufficient capability**: SQL generation is a well-defined task that doesn't require GPT-4's advanced reasoning
- **Iteration speed**: During development, cheaper models mean faster experimentation

For production with complex queries, I'd A/B test GPT-4 to measure if the quality improvement justifies the cost increase."

---

### 2. How Do You Prevent SQL Injection?

**Answer**:
"Multiple layers of protection:

1. **Whitelist approach**: Block dangerous SQL keywords (DROP, DELETE, UPDATE, INSERT, ALTER, CREATE)
```python
forbidden = ['DROP', 'DELETE', 'INSERT', 'UPDATE', 'ALTER', 'CREATE']
if any(word in sql.upper() for word in forbidden):
    return error
```

2. **Read-only queries**: Only allow SELECT statements

3. **Parameterized queries**: When using user input in SQL, use SQLAlchemy's parameter binding:
```python
# BAD: f"SELECT * FROM users WHERE name = '{user_input}'"
# GOOD: text("SELECT * FROM users WHERE name = :name").bindparams(name=user_input)
```

4. **AI validation**: The AI itself is instructed to generate safe SQL, adding another layer

5. **Future**: Could add query cost estimation to prevent expensive queries (e.g., full table scans)"

---

### 3. How Do You Handle OpenAI API Failures?

**Answer**:
"Comprehensive error handling:

```python
try:
    response = client.chat.completions.create(...)
    return {"success": True, "answer": response.choices[0].message.content}

except openai.RateLimitError:
    return {"success": False, "error": "Rate limit exceeded. Try again later."}

except openai.AuthenticationError:
    return {"success": False, "error": "Invalid API key"}

except openai.APIConnectionError:
    return {"success": False, "error": "Network connection failed"}

except Exception as e:
    return {"success": False, "error": f"Unexpected error: {str(e)}"}
```

In production, I'd add:
- **Retry logic** with exponential backoff
- **Fallback responses** when API is down
- **Circuit breaker pattern** to prevent cascading failures
- **Monitoring** to track error rates"

---

### 4. How Would You Scale This?

**Answer**:
"Several approaches depending on scale:

**Database Scaling**:
- Migrate from SQLite to PostgreSQL/MySQL for concurrent writes
- Add read replicas for high query volume
- Implement connection pooling
- Cache frequent queries (Redis)

**API Scaling**:
- Add load balancer (nginx)
- Horizontal scaling with multiple Flask instances (Gunicorn workers)
- Implement rate limiting per user
- Add request queuing for expensive operations

**AI Scaling**:
- Cache common SQL queries generated by AI
- Batch multiple questions in one API call
- Use streaming for long responses
- Fine-tune a smaller model for SQL generation (reduce API costs)

**Observability**:
- Add logging (Python logging module)
- Metrics (Prometheus)
- Distributed tracing (Jaeger)
- Dashboard (Grafana)"

---

### 5. How Do You Test AI-Generated SQL?

**Answer**:
"Multi-layered testing approach:

1. **Unit tests**: Test SQL validation functions
```python
def test_sql_safety():
    assert is_safe_query("SELECT * FROM products") == True
    assert is_safe_query("DROP TABLE products") == False
```

2. **Integration tests**: Test end-to-end flow with known questions
```python
def test_text_to_sql():
    question = "What are total sales?"
    response = ask_endpoint(question)
    assert "sql" in response
    assert "SELECT SUM" in response["sql"]
```

3. **Golden dataset**: Curate 50-100 question/SQL pairs, measure accuracy
```python
golden_tests = [
    ("total sales", "SELECT SUM(total) FROM sales"),
    ("top 5 products", "SELECT name FROM products ORDER BY price DESC LIMIT 5")
]
```

4. **Dry run mode**: Generate SQL but don't execute, review before enabling

5. **Monitoring**: Log all generated SQL, review failures, add to test suite"

---

## ‚ùì Common Interview Questions

### General Questions

**Q: Why did you build this project?**

**A**: "I wanted to learn how to build production AI agents, not just call OpenAI's API. This project teaches the full stack: prompt engineering, SQL generation, safety validation, API design, and database integration. Text-to-SQL is also a common real-world use case - businesses need to democratize data access for non-technical users."

---

**Q: What was the hardest part?**

**A**: "Prompt engineering for reliable SQL generation. You need to:
- Include the full schema so AI knows what tables/columns exist
- Provide examples of good queries
- Handle ambiguous questions ('sales' could mean revenue, quantity, or transaction count)
- Ensure generated SQL is syntactically valid
- Balance token limits (large schemas can exceed context windows)

I solved this by creating a structured schema representation and including concrete examples in the system prompt."

---

**Q: How would you improve this project?**

**A**:
1. **Add streaming responses**: Show SQL generation in real-time
2. **Query explanation**: AI explains what the SQL does before executing
3. **Query visualization**: Auto-generate charts for numeric results
4. **Conversation memory**: Remember previous questions for follow-ups
5. **Query optimization**: AI analyzes slow queries and suggests indexes
6. **Multi-database support**: Work with PostgreSQL, MySQL, etc.
7. **Fine-tuning**: Train a smaller, faster model specifically for our schema"

---

### Technical Questions

**Q: Walk me through the request flow for '/api/ask'**

**A**:
```
1. Client sends POST request with {"question": "What are total sales?"}

2. Flask route handler extracts question from request.json

3. Calls ai_sales_assistant(question)

4. Function builds system prompt:
   - "You are a SQL expert..."
   - Includes database schema from get_schema_info()

5. Calls OpenAI API:
   - Model: gpt-3.5-turbo
   - Messages: [system prompt, user question]
   - Max tokens: 500

6. OpenAI returns SQL: "SELECT SUM(total) FROM sales"

7. Validate SQL with is_safe_query()

8. Execute via run_query() if safe

9. Format results into JSON response

10. Return to client with status, answer, SQL used, and execution time
```

---

**Q: How do you handle database migrations?**

**A**: "Currently using SQLAlchemy's declarative base with `Base.metadata.create_all()` for simple schema creation. For production:

1. **Use Alembic** (SQLAlchemy's migration tool)
2. **Version control migrations**: Track schema changes in git
3. **Generate migrations**: `alembic revision --autogenerate`
4. **Apply migrations**: `alembic upgrade head`
5. **Rollback support**: `alembic downgrade -1`

Example migration:
```python
def upgrade():
    op.add_column('sales', sa.Column('customer_id', sa.Integer))

def downgrade():
    op.drop_column('sales', 'customer_id')
```

This enables zero-downtime deploys and safe schema evolution."

---

**Q: Explain your error handling strategy**

**A**: "Layered error handling at each boundary:

**API Layer** (app.py):
- Try/catch around all routes
- Return appropriate HTTP status codes (400, 500)
- Log errors for debugging
```python
@app.route('/api/ask', methods=['POST'])
def ask():
    try:
        question = request.json.get('question')
        if not question:
            return jsonify({"error": "Missing question"}), 400
        result = ai_sales_assistant(question)
        return jsonify(result), 200
    except Exception as e:
        return jsonify({"error": "Internal error"}), 500
```

**OpenAI Layer** (openai_service.py):
- Catch specific OpenAI exceptions
- Return structured error objects
- Graceful degradation
```python
except openai.RateLimitError:
    return {"success": False, "error": "Rate limit"}
```

**Database Layer** (database.py):
- Validate SQL before execution
- Connection error handling
- Transaction rollback on failures

**User-Facing**:
- Never expose internal errors
- Provide actionable messages
- Log detailed errors server-side"

---

**Q: How would you add authentication?**

**A**: "Depends on use case:

**Option 1: API Key Authentication** (simple)
```python
@app.before_request
def check_api_key():
    api_key = request.headers.get('X-API-Key')
    if api_key != os.getenv('VALID_API_KEY'):
        return jsonify({"error": "Unauthorized"}), 401
```

**Option 2: JWT Tokens** (standard)
```python
from flask_jwt_extended import jwt_required, get_jwt_identity

@app.route('/api/ask', methods=['POST'])
@jwt_required()
def ask():
    user_id = get_jwt_identity()
    # ... process request ...
```

**Option 3: OAuth2** (enterprise)
- Use OAuth provider (Google, GitHub)
- Implement authorization code flow
- Store tokens securely

I'd also add:
- Rate limiting per user
- Usage tracking (for billing)
- Role-based access control (admin vs user)"

---

## üìä Key Metrics to Mention

### Performance
- **API response time**: ~200ms for simple queries (database only)
- **AI response time**: ~2-5 seconds (OpenAI API call)
- **Database size**: 20KB (200 records)
- **Concurrent users**: Currently single-threaded Flask dev server (production would use Gunicorn with 4-8 workers)

### Code Quality
- **Total lines**: ~760 lines of Python
  - app.py: 219 lines
  - database.py: 235 lines
  - openai_service.py: 306 lines
- **Documentation**: Comprehensive docstrings on all functions
- **Error handling**: Try/catch on all external calls
- **Test coverage**: Built-in test functions (would add pytest for production)

### AI Metrics
- **Model**: gpt-3.5-turbo
- **Average tokens per request**: 200-500
- **Estimated cost per query**: $0.0003-$0.0008
- **Token limits**: 500 max completion tokens

---

## üèÜ Strengths to Highlight

1. **Clean Architecture**: Separation of concerns (routes, database, AI service)
2. **Security-First**: SQL injection prevention, API key management
3. **Production Patterns**: Error handling, logging, health checks
4. **Scalability Considerations**: Designed for easy migration to PostgreSQL, cloud deployment
5. **Documentation**: Extensive comments explaining learning concepts
6. **Iterative Development**: Modular approach, each module builds on previous

---

## üö® Weaknesses & How You'd Fix Them

**Weakness 1: No authentication**
- **Fix**: Add JWT tokens with Flask-JWT-Extended

**Weakness 2: Single-threaded Flask dev server**
- **Fix**: Deploy with Gunicorn + nginx in production

**Weakness 3: No caching**
- **Fix**: Add Redis for query result caching

**Weakness 4: Limited error recovery**
- **Fix**: Add retry logic with exponential backoff for API calls

**Weakness 5: No monitoring**
- **Fix**: Add Prometheus metrics and Grafana dashboards

**Weakness 6: SQLite not suitable for production**
- **Fix**: Migrate to PostgreSQL with connection pooling

---

## üé§ Closing Statement for Interviews

"This project demonstrates my ability to:
- Build full-stack applications with modern frameworks
- Integrate cutting-edge AI technologies into production systems
- Design secure APIs with proper error handling
- Work with databases and understand SQL optimization
- Think about scalability and production readiness
- Learn new technologies quickly and document my work

I'm excited to bring these skills to your team and continue building innovative AI-powered applications."

---

## üìö Additional Resources

- **Project Location**: `/Users/yus6/Documents/Courses/cs146s/agent/ai-agent/`
- **OpenAI Documentation**: https://platform.openai.com/docs
- **LangChain Docs**: https://python.langchain.com/docs
- **Flask Documentation**: https://flask.palletsprojects.com/
- **SQLAlchemy Tutorial**: https://docs.sqlalchemy.org/

---

**Last Updated**: January 2026
**Course**: CS146S - AI Agent Development
**Status**: Module 3 Complete, Module 4 In Progress
